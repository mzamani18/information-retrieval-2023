{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xZbwdiO7on8"
      },
      "source": [
        "# HW1 Solutions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mXTKt_R7on-"
      },
      "source": [
        "## Import libraries\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### `nltk`\n",
        "\n",
        "- **Description**: `Natural Language Toolkit`, I use it for tokenization, stemming , remove stop words and lemmatizer.\n",
        "\n",
        "##### `glob`\n",
        "\n",
        "- **Description**: Just for reading documents name in ./docs directory\n",
        "\n",
        "##### `string`\n",
        "\n",
        "- **Description**: I used from punctuations attribute on this package\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MeBGYRzd7on-"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import glob\n",
        "import string\n",
        "# nltk.download('stopwords')\n",
        "# nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qjC6tFE7on_"
      },
      "source": [
        "## Load Documents\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `LoadDocuments` class is responsible for loading and managing a collection of documents. If you want to change the document path please cgange path variable.\n",
        "\n",
        "##### Properties\n",
        "\n",
        "- **_`DOC_ID_MAPPER`_** _(class variable)_: A dictionary mapping document names to their respective IDs, Be care full, becuse it depends on your documents order and docIds started from zero.\n",
        "\n",
        "##### Methods\n",
        "\n",
        "##### `__init__(self)`\n",
        "\n",
        "- **Description**: Initializes a `LoadDocuments` object with a default path to load text that you can change it base on your docs directory path and an empty `documentCollection`. that is a dictionary.\n",
        "\n",
        "##### `Load(self)`\n",
        "\n",
        "- **Description**: Loads text documents from the specified path and populates the `documentCollection` property.\n",
        "\n",
        "##### `buildDocumentIDMapper(self)`\n",
        "\n",
        "- **Description**: Builds a document ID mapper by associating each document's name with a unique ID and populates the `DOC_ID_MAPPER` dictionary, I will print this mapper data for checking my results base on this mapper.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "bLflR9ig7on_"
      },
      "outputs": [],
      "source": [
        "class LoadDocuments:\n",
        "\n",
        "    DOC_ID_MAPPER = {}\n",
        "\n",
        "    def __init__(self) -> None:\n",
        "        self.path = \"./docs/*.txt\"\n",
        "        self.documentCollection = {}\n",
        "\n",
        "    def Load(self)->None:\n",
        "        documents = glob.glob(self.path)\n",
        "        for documentName in documents :\n",
        "            document = open(documentName, \"r\", encoding = 'cp1252')\n",
        "            self.documentCollection[documentName] = document.read()\n",
        "\n",
        "    def buildDocumentIDMapper(self)->None:\n",
        "        files = glob.glob(self.path)\n",
        "        for docId, documentName in enumerate(files):\n",
        "            self.DOC_ID_MAPPER[documentName] = docId\n",
        "            print(docId , ' -> ', documentName[7:])  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3ncGZTH7on_"
      },
      "source": [
        "## Document Preprocessing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `DocumentPreProcessor` class extends the functionality of the `LoadDocuments` class by providing methods to preprocess text documents. These methods perform various text processing tasks, such as converting text to lowercase, tokenization, removing punctuations, removing stop words, stemming, and lemmatization.\n",
        "\n",
        "#### Methods\n",
        "\n",
        "##### `__init__(self)`\n",
        "\n",
        "- **Description**: Initializes a `DocumentPreProcessor` object by calling the constructor of the base class `LoadDocuments`.\n",
        "\n",
        "##### `convertToLower(self)`\n",
        "\n",
        "- **Description**: Converts all text in the loaded documents to lowercase.\n",
        "\n",
        "##### `tokenizer(self)`\n",
        "\n",
        "- **Description**: Tokenizes the text in the loaded documents using a regular expression pattern.\n",
        "\n",
        "##### `removePunctuations(self)`\n",
        "\n",
        "- **Description**: Removes punctuations from the tokenized text in the loaded documents.\n",
        "\n",
        "##### `removeStopWords(self)`\n",
        "\n",
        "- **Description**: Removes stop words from the tokenized text in the loaded documents using NLTK's English stop words list.\n",
        "\n",
        "##### `stemming(self)`\n",
        "\n",
        "- **Description**: Applies stemming to the tokenized text in the loaded documents using the Porter Stemmer algorithm from NLTK.\n",
        "\n",
        "##### `lemmatizer(self)`\n",
        "\n",
        "- **Description**: Applies lemmatization to the tokenized text in the loaded documents using NLTK's WordNet Lemmatizer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "3bpbrLSa7on_"
      },
      "outputs": [],
      "source": [
        "class DocumentPreProcessor(LoadDocuments):\n",
        "    def __init__(self)->None:\n",
        "        super().__init__()\n",
        "\n",
        "    def convertToLower(self)->None:\n",
        "        for documentName, document in self.documentCollection.items():\n",
        "            self.documentCollection[documentName] = document.lower()\n",
        "\n",
        "    def tokenizer(self)->None:\n",
        "        pattern = r'\\d{1,3}(?:,\\d{3})*(?:\\.\\d+)?|\\w+'\n",
        "        for documentName, document in self.documentCollection.items():\n",
        "            self.documentCollection[documentName] = nltk.tokenize.regexp_tokenize(document, pattern)\n",
        "\n",
        "    def removePunctuations(self)->None:\n",
        "        for documentName, document in self.documentCollection.items():\n",
        "            for ind, term in enumerate(document):\n",
        "                self.documentCollection[documentName][ind] = \"\".join([i for i in term if i not in string.punctuation])\n",
        "\n",
        "    def removeStopWords(self)->None:\n",
        "        stopwords = nltk.corpus.stopwords.words('english')\n",
        "        for documentName, document in self.documentCollection.items():\n",
        "            self.documentCollection[documentName] = [i for i in document if i not in stopwords]\n",
        "\n",
        "    def stemming(self)->None:\n",
        "        porter_stemmer = nltk.stem.porter.PorterStemmer()\n",
        "        for documentName, document in self.documentCollection.items():\n",
        "            self.documentCollection[documentName] = [porter_stemmer.stem(term) for term in document]\n",
        "\n",
        "    def lemmatizer(self)->None:\n",
        "        wordnet_lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "        for documentName, document in self.documentCollection.items():\n",
        "            self.documentCollection[documentName] = [wordnet_lemmatizer.lemmatize(term) for term in document]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZONq2Dx7ooA"
      },
      "source": [
        "## Inverted Index\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `InvertedIndex` class will generate a invertedIndexex that maps terms to the documents that contain them, I will save all positions for proximate search .\n",
        "\n",
        "#### Properties\n",
        "\n",
        "- **`invertedIndex`**: A dictionary representing the inverted index where terms are mapped to document IDs and their positions.\n",
        "\n",
        "#### Methods\n",
        "\n",
        "##### `__init__(self)`\n",
        "\n",
        "- **Description**: Initializes an `InvertedIndex` object and call base class constructor.\n",
        "\n",
        "##### `buildInvertedIndex(self)`\n",
        "\n",
        "- **Description**: Builds the inverted index by iterating through preprocessed documents and mapping terms to document IDs and their positions.\n",
        "\n",
        "##### `build(self) -> None`\n",
        "\n",
        "- **Description**: Initiates the preprocessing steps by loading documents, building document ID mapper, converting to lowercase, tokenizing, removing punctuations, removing stop words, stemming, lemmatizing, and finally, building the inverted index.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "3BvbHTGH7ooA"
      },
      "outputs": [],
      "source": [
        "class InvertedIndex(DocumentPreProcessor):\n",
        "    def __init__(self) -> None:\n",
        "        super().__init__()\n",
        "        self.invertedIndex = {}    \n",
        "\n",
        "    def buildInvertedIndex(self)->None:\n",
        "        for documentName, document in self.documentCollection.items():\n",
        "            for ind, term in enumerate(document):\n",
        "                if term not in self.invertedIndex:\n",
        "                    self.invertedIndex[term] = {}\n",
        "\n",
        "                if(self.DOC_ID_MAPPER[documentName] not in self.invertedIndex[term]):\n",
        "                    self.invertedIndex[term][self.DOC_ID_MAPPER[documentName]] = []\n",
        "\n",
        "                self.invertedIndex[term][self.DOC_ID_MAPPER[documentName]].append(ind)\n",
        "\n",
        "    def build(self)->None:\n",
        "        self.Load()\n",
        "        self.buildDocumentIDMapper()\n",
        "        self.convertToLower()\n",
        "        self.tokenizer()\n",
        "        self.removePunctuations()\n",
        "        self.removeStopWords()\n",
        "        self.stemming()\n",
        "        self.lemmatizer()\n",
        "        self.buildInvertedIndex()      "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VnCRpLLc7ooA"
      },
      "source": [
        "## Query Processing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `QueryPreProcessing` class provides methods to preprocess search queries. It is like document preprocessing because I want to make them similar but I don't remove punctuations.\n",
        "\n",
        "#### Properties\n",
        "\n",
        "- **`query`**: the Query string that I gave from user\n",
        "- **`queryType`**: A string indicating the type of query (`AND`, `OR`, `NOT`, or `NEAR`), I keep my query type and then I will remove it from my query.\n",
        "- **`maxDistance`**: An integer representing the maximum distance for `NEAR` queries.\n",
        "\n",
        "#### Methods\n",
        "\n",
        "##### `__init__(self, query:str)`\n",
        "\n",
        "- **Description**: Initializes a `QueryPreProcessing` object with the provided query string.\n",
        "\n",
        "##### `determineQueryType(self)`\n",
        "\n",
        "- **Description**: Determines the query type based on the query string and sets the `queryType` and `maxDistance` properties.\n",
        "\n",
        "##### `convertToLower(self)`\n",
        "\n",
        "- **Description**: Converts all query terms to lowercase.\n",
        "\n",
        "##### `removePunctuations(self)`\n",
        "\n",
        "- **Description**: Removes punctuations from query terms.\n",
        "\n",
        "##### `removeQueryEXP(self)`\n",
        "\n",
        "- **Description**: Removes specific query expressions (`AND`, `OR`, `NOT`, and `NEAR`) from the query.\n",
        "\n",
        "##### `stemming(self)`\n",
        "\n",
        "- **Description**: Stems query terms using the Porter Stemmer algorithm.\n",
        "\n",
        "##### `lemmatizer(self)`\n",
        "\n",
        "- **Description**: Lemmatizes query terms using the WordNet Lemmatizer.\n",
        "\n",
        "##### `execute(self)`\n",
        "\n",
        "- **Description**: Executes the entire preprocessing for generating a clean query.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "O4xEFDLB7ooA"
      },
      "outputs": [],
      "source": [
        "class QueryPreProcessing:\n",
        "\n",
        "    ALL_QUERY_TYPES = [\n",
        "        \"and\",\n",
        "        \"or\",\n",
        "        \"not\",\n",
        "        \"near\"\n",
        "    ]\n",
        "\n",
        "    def __init__(self, query:str) -> None:\n",
        "        self.query = query.split(\" \")\n",
        "        self.queryType = \"\"\n",
        "        self.maxDistance = 0\n",
        "\n",
        "    def determineQueryType(self)->None:\n",
        "        if self.query[1] == \"AND\":\n",
        "            self.queryType = \"AND\"\n",
        "        elif self.query[1] == \"OR\":\n",
        "            self.queryType = \"OR\"\n",
        "        elif self.query[0] == \"NOT\":\n",
        "            self.queryType = \"NOT\"\n",
        "        else:\n",
        "            self.queryType = \"NEAR\"\n",
        "            self.maxDistance = int(self.query[1].split(\"/\")[1])   \n",
        "\n",
        "    def convertToLower(self)->None:\n",
        "            for ind, term in enumerate(self.query):\n",
        "                self.query[ind] = term.lower()\n",
        "\n",
        "    def removePunctuations(self) ->None:\n",
        "        for ind, term in enumerate(self.query):\n",
        "            self.query[ind] =\"\".join([i for i in term if i not in string.punctuation])           \n",
        "\n",
        "    def removeQueryEXP(self)->None:\n",
        "        self.query = [term for term in self.query if term not in self.ALL_QUERY_TYPES and not term.startswith(\"NEAR\")]\n",
        "\n",
        "    def stemming(self)->None:\n",
        "        porter_stemmer = nltk.stem.porter.PorterStemmer()\n",
        "        self.query = [porter_stemmer.stem(term) for term in self.query]\n",
        "\n",
        "    def lemmatizer(self)->None:\n",
        "        wordnet_lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "        self.query = [wordnet_lemmatizer.lemmatize(term) for term in self.query]  \n",
        "\n",
        "    def execute(self)->None:\n",
        "        self.determineQueryType()\n",
        "        self.convertToLower()\n",
        "        self.removePunctuations()  \n",
        "        self.removeQueryEXP()\n",
        "        self.stemming()\n",
        "        self.lemmatizer()       "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Utils\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `Utils` class contains utility methods for set operations like intersection and union and proximity distance calculations that is a little custom function(maybe I should change it's name).\n",
        "\n",
        "#### Methods\n",
        "\n",
        "##### `interSect(ls: list)`\n",
        "\n",
        "- **Description**: Calculates the intersection of two lists.\n",
        "\n",
        "##### `union(ls: list)`\n",
        "\n",
        "- **Description**: Calculates the union of two lists.\n",
        "\n",
        "##### `complement(ls: list)`\n",
        "\n",
        "- **Description**: Calculates the complement of two sets.\n",
        "\n",
        "##### `proximateDistanceCal(ls: list, maxDistance: int)`\n",
        "\n",
        "- **Description**: Calculates proximate distances within the given maximum distance between indexes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Utils:\n",
        "    def __init__(self) -> None:\n",
        "        pass\n",
        "\n",
        "    def interSect(ls: list)->list:\n",
        "        return list(set([i for i in ls[0] if i in ls[1]]))\n",
        "    \n",
        "    def union(ls :list)->list:\n",
        "        return list(set(ls[0] + ls[1]))\n",
        "    \n",
        "    def complement(ls: list)->list:\n",
        "        return list(set([i for i in ls[1].values() if i not in ls[0]]))\n",
        "    \n",
        "    def proximateDistanceCal(ls: list, maxDistance: int)->list:\n",
        "        result = []\n",
        "        for docId, indexes in ls[0].items():\n",
        "            if(docId in ls[1]):\n",
        "                for i in range (len(indexes)-1):\n",
        "                    if(abs(indexes[i] - indexes[i+1]) <= maxDistance + 1):\n",
        "                        result.append(docId)\n",
        "                for i in range(len(ls[1][docId])-1):\n",
        "                    if(abs(ls[1][docId][i] - ls[1][docId][i+1]) <= maxDistance + 1):\n",
        "                        result.append(docId)     \n",
        "\n",
        "        return list(set(result))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## IR System\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `IR` class is my information retrieval system that will generate InvertedInxe and will get your query and generate output, If you want a IR system that get many queries and be up for a while you can put start function in while.\n",
        "\n",
        "#### Methods\n",
        "\n",
        "##### `booleanSearch(query: QueryPreProcessing)`\n",
        "\n",
        "- **Description**: Performs boolean search operations (AND, OR, NOT) based on the provided query.\n",
        "\n",
        "##### `proximateSearch(query: QueryPreProcessing)`\n",
        "\n",
        "- **Description**: Performs proximate search operations based on the provided query and maximum distance.\n",
        "\n",
        "##### `search(query: QueryPreProcessing)`\n",
        "\n",
        "- **Description**: Determines the type of search operation (boolean or proximate) based on the query type and performs the search.\n",
        "\n",
        "##### `start()`\n",
        "\n",
        "- **Description**: Initiates the information retrieval process by taking user input for the query, processing it, and displaying the resulting document IDs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "XKg39uYw7ooB",
        "outputId": "e1c85b3e-b2a1-4945-fa37-694a12905a1a"
      },
      "outputs": [],
      "source": [
        "class IR:\n",
        "    def __init__(self) -> None:\n",
        "        self.invertedIndex = InvertedIndex()\n",
        "        self.invertedIndex.build()\n",
        "        self.utils = Utils()\n",
        "\n",
        "    def booleanSearch(self, query: QueryPreProcessing)->list:\n",
        "        result_docs = []\n",
        "        for term in query.query:\n",
        "            if term in self.invertedIndex.invertedIndex:\n",
        "                result_docs.append(list(self.invertedIndex.invertedIndex[term].keys()))\n",
        "            else: result_docs.append([])\n",
        "            \n",
        "        if(query.queryType == \"AND\"): return Utils.interSect(result_docs)\n",
        "        elif (query.queryType == \"OR\") : return Utils.union(result_docs)\n",
        "        elif(query.queryType == \"NOT\") : \n",
        "            result_docs.append(InvertedIndex.DOC_ID_MAPPER)\n",
        "            return Utils.complement(result_docs)\n",
        "            \n",
        "    def proximateSearch(self, query: QueryPreProcessing)->list:\n",
        "        result_docs = []\n",
        "        for term in query.query:\n",
        "            if term in self.invertedIndex.invertedIndex:\n",
        "                result_docs.append(self.invertedIndex.invertedIndex[term])\n",
        "\n",
        "        return Utils.proximateDistanceCal(result_docs, query.maxDistance)\n",
        "            \n",
        "    def search(self, query: QueryPreProcessing)-> list:\n",
        "        if(query.queryType != \"NEAR\"):\n",
        "            return self.booleanSearch(query)\n",
        "        else:\n",
        "            return self.proximateSearch(query)   \n",
        "        \n",
        "    def start(self)->None:\n",
        "        query = input(\"Enter Your Query: '\\n'\")   \n",
        "        print('\\n') \n",
        "        print(\"Input query : \" , query)\n",
        "        query = QueryPreProcessing(query)\n",
        "        query.execute()\n",
        "        print(\"Result document Ids : \" , self.search(query))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "C29CtM9U7ooB"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0  ->  Jerry Decided To Buy a Gun.txt\n",
            "1  ->  Gasoline Prices Hit Record High.txt\n",
            "2  ->  Man Injured at Fast Food Place.txt\n",
            "3  ->  Freeway Chase Ends at Newsstand.txt\n",
            "4  ->  Happy and Unhappy Renters.txt\n",
            "5  ->  A Festival of Books.txt\n",
            "6  ->  Cloning Pets.txt\n",
            "7  ->  Rentals at the Oceanside Community.txt\n",
            "8  ->  Crazy Housing Prices.txt\n",
            "9  ->  A Murder-Suicide.txt\n",
            "10  ->  Pulling Out Nine Tons of Trash.txt\n",
            "11  ->  Food Fight Erupted in Prison.txt\n",
            "12  ->  Sara Went Shopping.txt\n",
            "13  ->  Trees Are a Threat.txt\n",
            "14  ->  Better To Be Unlucky.txt\n",
            "\n",
            "\n",
            "Input query :  decision AND run\n",
            "Result document Ids :  [8, 14]\n"
          ]
        }
      ],
      "source": [
        "IR = IR()\n",
        "IR.start()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Report:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Steps:\n",
        "\n",
        "1. For this Mini project at first I tried to read all files from google drive but I faced some issue in urls, because it didn't read all directory and then I faced some issues, then I decided to download all files and put the in a local directory.\n",
        "\n",
        "2. After I write my document loader class I read a little about nltk for preprocessing because all of us know about removing punctuations and some thing like this but for stemmin and lemmatizer I didn't have any idea, so I read about it and nltk as a tool for doing these.\n",
        "\n",
        "3. In this step I decided to make my invertedIndex and as you know we have dictionary in python that make this step ver easy, becuse I decided to implement my trie and ... but dictionary is like a hash table and is very efficient, so I used it and build my invertedIndex\n",
        "\n",
        "4. I write my QueryPreprocessor, but it's like my document preprocessor.\n",
        "\n",
        "5. I assembled all these component and in this step I forced to write a Util for giving some functionality to me, then I faced a problem because at first I insterted touples like (docId, position), but ut was a little bad and I faced problem for searching in them, so I convetet it to a set like docId -> [positions ... ]. this step was the most critical and challenging section for me.\n",
        "\n",
        "6. In this step I used some queries to check my code.\n",
        "\n",
        "7. In last step I write a documentation for my code, but for better style and a formal language I gave them and my classes to chatGpt I helped me to generate documents, at last I revise all things and merge all of them and write my document.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## References:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. Our course slides\n",
        "2. https://www.nltk.org/\n",
        "3. https://chat.openai.com/\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
