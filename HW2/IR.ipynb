{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xZbwdiO7on8"
      },
      "source": [
        "# HW2 Solutions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mXTKt_R7on-"
      },
      "source": [
        "## Import libraries\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### `nltk`\n",
        "\n",
        "- **Description**: `Natural Language Toolkit`, I use it for tokenization, stemming , remove stop words and lemmatizer.\n",
        "\n",
        "##### `glob`\n",
        "\n",
        "- **Description**: Just for reading documents name in ./docs directory\n",
        "\n",
        "##### `string`\n",
        "\n",
        "- **Description**: I used from punctuations attribute on this package\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MeBGYRzd7on-"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import glob\n",
        "import string\n",
        "# nltk.download('stopwords')\n",
        "# nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qjC6tFE7on_"
      },
      "source": [
        "## Load Documents\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `LoadDocuments` class is responsible for loading and managing a collection of documents. If you want to change the document path please cgange path variable.\n",
        "\n",
        "##### Properties\n",
        "\n",
        "- **_`DOC_ID_MAPPER`_** _(class variable)_: A dictionary mapping document names to their respective IDs, Be care full, becuse it depends on your documents order and docIds started from zero.\n",
        "\n",
        "##### Methods\n",
        "\n",
        "##### `__init__(self)`\n",
        "\n",
        "- **Description**: Initializes a `LoadDocuments` object with a default path to load text that you can change it base on your docs directory path and an empty `documentCollection`. that is a dictionary.\n",
        "\n",
        "##### `Load(self)`\n",
        "\n",
        "- **Description**: Loads text documents from the specified path and populates the `documentCollection` property.\n",
        "\n",
        "##### `buildDocumentIDMapper(self)`\n",
        "\n",
        "- **Description**: Builds a document ID mapper by associating each document's name with a unique ID and populates the `DOC_ID_MAPPER` dictionary, I will print this mapper data for checking my results base on this mapper.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "bLflR9ig7on_"
      },
      "outputs": [],
      "source": [
        "class LoadDocuments:\n",
        "\n",
        "    DOC_ID_MAPPER = {}\n",
        "\n",
        "    def __init__(self) -> None:\n",
        "        self.path = \"./docs/*.txt\"\n",
        "        self.documentCollection = {}\n",
        "\n",
        "    def Load(self)->None:\n",
        "        documents = glob.glob(self.path)\n",
        "        for documentName in documents :\n",
        "            document = open(documentName, \"r\", encoding = 'cp1252')\n",
        "            self.documentCollection[documentName] = document.read()\n",
        "\n",
        "    def buildDocumentIDMapper(self)->None:\n",
        "        files = glob.glob(self.path)\n",
        "        for docId, documentName in enumerate(files):\n",
        "            self.DOC_ID_MAPPER[documentName] = docId\n",
        "            print(docId , ' -> ', documentName[7:])  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3ncGZTH7on_"
      },
      "source": [
        "## Document Preprocessing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `DocumentPreProcessor` class extends the functionality of the `LoadDocuments` class by providing methods to preprocess text documents. These methods perform various text processing tasks, such as converting text to lowercase, tokenization, removing punctuations, removing stop words, stemming, and lemmatization.\n",
        "\n",
        "##### Properties\n",
        "\n",
        "- **_`correctWords`_** _(class variable)_: As question said we assume that terms in our documents are correst, so I will add all terms in my documents ro this array because I need it for miss spelling\n",
        "\n",
        "#### Methods\n",
        "\n",
        "##### `__init__(self)`\n",
        "\n",
        "- **Description**: Initializes a `DocumentPreProcessor` object by calling the constructor of the base class `LoadDocuments`.\n",
        "\n",
        "##### `convertToLower(self)`\n",
        "\n",
        "- **Description**: Converts all text in the loaded documents to lowercase.\n",
        "\n",
        "##### `tokenizer(self)`\n",
        "\n",
        "- **Description**: Tokenizes the text in the loaded documents using a regular expression pattern.\n",
        "\n",
        "##### `removePunctuations(self)`\n",
        "\n",
        "- **Description**: Removes punctuations from the tokenized text in the loaded documents.\n",
        "\n",
        "##### `removeStopWords(self)`\n",
        "\n",
        "- **Description**: Removes stop words from the tokenized text in the loaded documents using NLTK's English stop words list.\n",
        "\n",
        "##### `buildCorrectWordsList(self)`\n",
        "\n",
        "- **Description**: Append all terms in all documents to this list a correct word hub\n",
        "\n",
        "##### `stemming(self)`\n",
        "\n",
        "- **Description**: Applies stemming to the tokenized text in the loaded documents using the Porter Stemmer algorithm from NLTK.\n",
        "\n",
        "##### `lemmatizer(self)`\n",
        "\n",
        "- **Description**: Applies lemmatization to the tokenized text in the loaded documents using NLTK's WordNet Lemmatizer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "3bpbrLSa7on_"
      },
      "outputs": [],
      "source": [
        "class DocumentPreProcessor(LoadDocuments):\n",
        "\n",
        "    correctWords = []\n",
        "\n",
        "    def __init__(self)->None:\n",
        "        super().__init__()\n",
        "\n",
        "    def convertToLower(self)->None:\n",
        "        for documentName, document in self.documentCollection.items():\n",
        "            self.documentCollection[documentName] = document.lower()\n",
        "\n",
        "    def tokenizer(self)->None:\n",
        "        pattern = r'\\d{1,3}(?:,\\d{3})*(?:\\.\\d+)?|\\w+'\n",
        "        for documentName, document in self.documentCollection.items():\n",
        "            self.documentCollection[documentName] = nltk.tokenize.regexp_tokenize(document, pattern)\n",
        "\n",
        "    def removePunctuations(self)->None:\n",
        "        for documentName, document in self.documentCollection.items():\n",
        "            for ind, term in enumerate(document):\n",
        "                self.documentCollection[documentName][ind] = \"\".join([i for i in term if i not in string.punctuation])\n",
        "\n",
        "    def removeStopWords(self)->None:\n",
        "        stopwords = nltk.corpus.stopwords.words('english')\n",
        "        for documentName, document in self.documentCollection.items():\n",
        "            self.documentCollection[documentName] = [i for i in document if i not in stopwords]\n",
        "\n",
        "    def buildCorrectWordsList(self)->list:\n",
        "        self.correctWords = [term for _,document in self.documentCollection.items() for term in document]\n",
        "        \n",
        "    def stemming(self)->None:\n",
        "        porter_stemmer = nltk.stem.porter.PorterStemmer()\n",
        "        for documentName, document in self.documentCollection.items():\n",
        "            self.documentCollection[documentName] = [porter_stemmer.stem(term) for term in document]\n",
        "\n",
        "    def lemmatizer(self)->None:\n",
        "        wordnet_lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "        for documentName, document in self.documentCollection.items():\n",
        "            self.documentCollection[documentName] = [wordnet_lemmatizer.lemmatize(term) for term in document]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZONq2Dx7ooA"
      },
      "source": [
        "## Inverted Index\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `InvertedIndex` class will generate a invertedIndexex that maps terms to the documents that contain them, I will save all positions for proximate search .\n",
        "\n",
        "#### Properties\n",
        "\n",
        "- **`invertedIndex`**: A dictionary representing the inverted index where terms are mapped to document IDs and their positions.\n",
        "\n",
        "#### Methods\n",
        "\n",
        "##### `__init__(self)`\n",
        "\n",
        "- **Description**: Initializes an `InvertedIndex` object and call base class constructor.\n",
        "\n",
        "##### `buildInvertedIndex(self)`\n",
        "\n",
        "- **Description**: Builds the inverted index by iterating through preprocessed documents and mapping terms to document IDs and their positions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "3BvbHTGH7ooA"
      },
      "outputs": [],
      "source": [
        "class InvertedIndex(DocumentPreProcessor):\n",
        "\n",
        "    def __init__(self) -> None:\n",
        "        super().__init__()\n",
        "        self.invertedIndex = {}\n",
        "\n",
        "    def buildInvertedIndex(self)->None:\n",
        "        for documentName, document in self.documentCollection.items():\n",
        "            for ind, term in enumerate(document):\n",
        "                if term not in self.invertedIndex:\n",
        "                    self.invertedIndex[term] = {}\n",
        "\n",
        "                if(self.DOC_ID_MAPPER[documentName] not in self.invertedIndex[term]):\n",
        "                    self.invertedIndex[term][self.DOC_ID_MAPPER[documentName]] = []\n",
        "\n",
        "                self.invertedIndex[term][self.DOC_ID_MAPPER[documentName]].append(ind)\n",
        "                  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Trie Tree\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Represents a Trie data structure for storing and querying words.\n",
        "\n",
        "#### TrieNode Properties\n",
        "\n",
        "- `char`: Character stored in the node.\n",
        "- `isEnd`: Boolean value indicating if the node represents the end of a word.\n",
        "- `counter`: Integer count representing the number of words with the same prefix.\n",
        "- `child`: Dictionary containing child nodes.\n",
        "\n",
        "#### Properties\n",
        "\n",
        "- `root`: The root node of the Trie.\n",
        "\n",
        "#### Methods\n",
        "\n",
        "##### `__init__(self)`\n",
        "\n",
        "Constructor method. Initializes a Trie object with an empty root node.\n",
        "\n",
        "##### `insert(self, string: str)`\n",
        "\n",
        "Inserts a word into the Trie.\n",
        "\n",
        "##### `query(self, x: str)`\n",
        "\n",
        "Queries the Trie for words matching the given prefix `x`. Returns a list of matching words.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TrieNode:\n",
        "    def __init__(self, char) -> None:\n",
        "        self.char = char\n",
        "        self.isEnd = False\n",
        "        self.counter = 0\n",
        "        self.child = {}\n",
        "\n",
        "class Trie():\n",
        "    def __init__(self) -> None:\n",
        "        self.root = TrieNode(\"\")\n",
        "\n",
        "    def insert(self, string :str)->None:\n",
        "        node = self.root\n",
        "\n",
        "        for char in string:\n",
        "            if char in node.child:\n",
        "                node = node.child[char]\n",
        "            else:\n",
        "                new_node = TrieNode(char)\n",
        "                node.child[char] = new_node\n",
        "                node = new_node\n",
        "        node.isEnd = True\n",
        "        node.counter +=1\n",
        "\n",
        "    def dfs(self, node: TrieNode, prefix:str)->None:\n",
        "        if node.isEnd:\n",
        "            self.output.append(prefix + node.char)\n",
        "        \n",
        "        for child in node.child.values():\n",
        "            self.dfs(child, prefix + node.char)\n",
        "\n",
        "    def query(self, x):\n",
        "        self.output = []\n",
        "        node = self.root\n",
        "        \n",
        "        for char in x:\n",
        "            if char in node.child:\n",
        "                node = node.child[char]\n",
        "            else:\n",
        "                return []\n",
        "\n",
        "        self.dfs(node, x[:-1])\n",
        "        \n",
        "        return self.output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Permuterm Index\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `PermutermIndex` class extends `DocumentPreProcessor` and provides functionality for building a Permuterm index and searching for wildcard queries in a collection of documents.\n",
        "\n",
        "#### Properties\n",
        "\n",
        "- `permutermIndex`: Dictionary containing permuterm rotations of words.\n",
        "- `trie`: Trie data structure for efficient wildcard query handling.\n",
        "\n",
        "#### Methods\n",
        "\n",
        "##### `__init__(self)`\n",
        "\n",
        "Constructor method. Initializes a `PermutermIndex` object with an empty `permutermIndex` dictionary and a Trie.\n",
        "\n",
        "##### `buildPermuteIndex(self)`\n",
        "\n",
        "Builds the permuterm index using the corrected words obtained from `DocumentPreProcessor`. Populates the `permutermIndex` dictionary and inserts permuterm rotations into the Trie.\n",
        "\n",
        "##### `searchWildcard(self, query: str)`\n",
        "\n",
        "Searches for wildcard queries in the permuterm index. Handles queries with one or two '\\*' symbols. Returns a list of matching words.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PermutermIndex(DocumentPreProcessor):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.permutermIndex = {}\n",
        "        self.trie = Trie()\n",
        "\n",
        "    def buildPermuteIndex(self)->None:\n",
        "        for term in self.correctWords:\n",
        "            term = term + \"$\"\n",
        "            rotations = [term[i:] + term[:i] for i in range(len(term))]\n",
        "            for rotation in rotations:\n",
        "                if rotation not in self.permutermIndex:\n",
        "                    self.permutermIndex[rotation] = []\n",
        "                self.permutermIndex[rotation].append(term[:-1])\n",
        "\n",
        "                self.trie.insert(rotation)            \n",
        "\n",
        "    def searchWildcard(self, query:string):\n",
        "        if(query.count(\"*\") == 1):\n",
        "            query = query + \"$\"\n",
        "            prefix, suffix = query.split(\"*\")\n",
        "            query = suffix + prefix\n",
        "            res = self.trie.query(query)\n",
        "            result = []\n",
        "            for term in res:\n",
        "                if term in self.permutermIndex:\n",
        "                    result.extend(self.permutermIndex[term])\n",
        "            return result\n",
        "\n",
        "        elif(query.count(\"*\") == 2):\n",
        "            query = query + \"$\"\n",
        "            prefix, mid , suffix = query.split(\"*\")\n",
        "            query = suffix + prefix\n",
        "            res = self.trie.query(query)\n",
        "            result = []\n",
        "            for term in res:\n",
        "                if term in self.permutermIndex :\n",
        "                    for word in self.permutermIndex[term]:\n",
        "                        if mid in word[len(prefix)-1:len(term)-len(suffix)]:\n",
        "                            result.append(word)\n",
        "            return list(set(result))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Indexes:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `Indexes` class will generate all indexes, it is a bridge between all types of indexes and will generate all of them.\n",
        "\n",
        "#### Methods\n",
        "\n",
        "##### `__init__(self)`\n",
        "\n",
        "- **Description**: Initializes base class constructor.\n",
        "\n",
        "##### `build(self) -> None`\n",
        "\n",
        "- **Description**: Initiates the preprocessing steps by loading documents, building document ID mapper, converting to lowercase, tokenizing, removing punctuations, removing stop words, and finally, building inverted index and wildCard index\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Indexes(PermutermIndex, InvertedIndex):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "    \n",
        "    def build(self)->None:\n",
        "        self.Load()\n",
        "        self.buildDocumentIDMapper()\n",
        "        self.convertToLower()\n",
        "        self.tokenizer()\n",
        "        self.removePunctuations()\n",
        "        self.removeStopWords()\n",
        "        self.buildCorrectWordsList()\n",
        "        # self.stemming()\n",
        "        # self.lemmatizer()\n",
        "        self.buildInvertedIndex()  \n",
        "        self.buildPermuteIndex()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Spelling Correction\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `SpellCorrection` class provides methods for correcting misspelled words using the Levenshtein distance algorithm.\n",
        "\n",
        "#### Methods\n",
        "\n",
        "##### `__init__(self)`\n",
        "\n",
        "Constructor method. Initializes a `SpellCorrection` object.\n",
        "\n",
        "##### `levenshteinDistanceCal(self, s1: str, s2: str) -> int`\n",
        "\n",
        "Calculates the Levenshtein distance between two strings `s1` and `s2`.\n",
        "\n",
        "##### `getCorrectWord(self, input: str, correctWords: list) -> str`\n",
        "\n",
        "Finds the correct word from a list of `correctWords` that is closest to the `input` string in terms of Levenshtein distance. for this I return just the first one but we can return a list of nearst words ...\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SpellCorrection:\n",
        "    def __init__(self) -> None:\n",
        "        pass\n",
        "\n",
        "    def levenshteinDistanceCal(self, s1: str, s2: str)->int:\n",
        "        m = [[0 for _ in range(len(s2)+1)] for _ in range(len(s1)+1)]\n",
        "\n",
        "        for i in range(1 , len(s1)+1):\n",
        "            m[i][0] = i\n",
        "        for j in range (1, len(s2)+1):\n",
        "            m[0][j] = j\n",
        "        for i in range(1, len(s1)+1):\n",
        "            for j in range(1, len(s2)+1):\n",
        "                tmp = 1\n",
        "                if s1[i-1] == s2[j-1] :\n",
        "                    tmp = 0\n",
        "                m[i][j] = min(m[i-1][j-1]+ tmp, m[i-1][j]+1, m[i][j-1] + 1)\n",
        "       \n",
        "        return m[len(s1)][len(s2)]\n",
        "\n",
        "    def getCorrectWord(self, input :str, correctWords: list)->str:\n",
        "        levenshteinDistances = [self.levenshteinDistanceCal(s1, input) for s1 in correctWords]   \n",
        "        minDistance = min(levenshteinDistances)\n",
        "        return [correctWords[ind] for ind, dis in enumerate(levenshteinDistances) if dis == minDistance][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VnCRpLLc7ooA"
      },
      "source": [
        "## Query Processing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `QueryPreProcessing` class provides methods to preprocess search queries. It is like document preprocessing because I want to make them similar but I don't remove punctuations.\n",
        "\n",
        "#### Properties\n",
        "\n",
        "- **`query`**: the Query string that I gave from user\n",
        "- **`queryType`**: A string indicating the type of query (`AND`, `OR`, `NOT`, or `NEAR`), I keep my query type and then I will remove it from my query.\n",
        "- **`maxDistance`**: An integer representing the maximum distance for `NEAR` queries.\n",
        "\n",
        "#### Methods\n",
        "\n",
        "##### `__init__(self, query:str)`\n",
        "\n",
        "- **Description**: Initializes a `QueryPreProcessing` object with the provided query string.\n",
        "\n",
        "##### `determineQueryType(self)`\n",
        "\n",
        "- **Description**: Determines the query type based on the query string and sets the `queryType` and `maxDistance` properties.\n",
        "\n",
        "##### `convertToLower(self)`\n",
        "\n",
        "- **Description**: Converts all query terms to lowercase.\n",
        "\n",
        "##### `removePunctuations(self)`\n",
        "\n",
        "- **Description**: Removes punctuations from query terms.\n",
        "\n",
        "##### `removeQueryEXP(self)`\n",
        "\n",
        "- **Description**: Removes specific query expressions (`AND`, `OR`, `NOT`, and `NEAR`) from the query.\n",
        "\n",
        "##### `spellCorrection(self)`\n",
        "\n",
        "- **Description**: Check spell correction and I will use my SpellCorrection to remove miss spelling\n",
        "\n",
        "##### `stemming(self)`\n",
        "\n",
        "- **Description**: Stems query terms using the Porter Stemmer algorithm.\n",
        "\n",
        "##### `lemmatizer(self)`\n",
        "\n",
        "- **Description**: Lemmatizes query terms using the WordNet Lemmatizer.\n",
        "\n",
        "##### `execute(self)`\n",
        "\n",
        "- **Description**: Executes the entire preprocessing for generating a clean query.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "O4xEFDLB7ooA"
      },
      "outputs": [],
      "source": [
        "class QueryPreProcessing:\n",
        "\n",
        "    ALL_QUERY_TYPES = [\n",
        "        \"and\",\n",
        "        \"or\",\n",
        "        \"not\",\n",
        "        \"near\",\n",
        "        \"spellCorrection\",\n",
        "        \"wildCard\"\n",
        "    ]\n",
        "\n",
        "    def __init__(self, query:str, correctWords : list) -> None:\n",
        "        self.correctWords = correctWords\n",
        "        self.query = query.split(\" \")\n",
        "        self.queryType = \"\"\n",
        "        self.maxDistance = 0\n",
        "\n",
        "    def determineQueryType(self)->None:\n",
        "        if self.query[0] == \"NOT\":\n",
        "            self.queryType = \"NOT\"\n",
        "        elif len(self.query) == 1 and \"*\" in self.query[0]:\n",
        "            self.queryType = \"wildCard\"    \n",
        "        elif len(self.query) == 1 :\n",
        "            self.queryType = \"spellCorrection\" \n",
        "        elif self.query[1] == \"AND\":\n",
        "            self.queryType = \"AND\"\n",
        "        elif self.query[1] == \"OR\":\n",
        "            self.queryType = \"OR\"\n",
        "        elif self.query[1].startswith(\"NEAR\"):\n",
        "            self.queryType = \"NEAR\"\n",
        "            self.maxDistance = int(self.query[1].split(\"/\")[1])   \n",
        "        else:    \n",
        "            self.queryType = \"spellCorrection\"\n",
        "        \n",
        "\n",
        "    def convertToLower(self)->None:\n",
        "            for ind, term in enumerate(self.query):\n",
        "                self.query[ind] = term.lower()\n",
        "\n",
        "    def removePunctuations(self) ->None:\n",
        "        for ind, term in enumerate(self.query):\n",
        "            punctuations = r\"\"\"!\"#$%&'()+,-./:;<=>?@[\\]^_`{|}~\"\"\" # I removed * for wildcard\n",
        "            self.query[ind] =\"\".join([i for i in term if i not in punctuations])           \n",
        "\n",
        "    def removeQueryEXP(self)->None:\n",
        "        self.query = [term for term in self.query if term not in self.ALL_QUERY_TYPES and not term.startswith(\"NEAR\")]    \n",
        "\n",
        "    def spellCorrection(self)->None:\n",
        "        spellCorrection = SpellCorrection()\n",
        "        for ind, term in enumerate(self.query):\n",
        "            if \"*\" not in term:\n",
        "                self.query[ind] = spellCorrection.getCorrectWord(term, self.correctWords)  \n",
        "\n",
        "    def stemming(self)->None:\n",
        "        porter_stemmer = nltk.stem.porter.PorterStemmer()\n",
        "        self.query = [porter_stemmer.stem(term) for term in self.query]\n",
        "\n",
        "    def lemmatizer(self)->None:\n",
        "        wordnet_lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "        self.query = [wordnet_lemmatizer.lemmatize(term) for term in self.query]  \n",
        "\n",
        "    def execute(self)->None:\n",
        "        self.determineQueryType()\n",
        "        self.convertToLower()\n",
        "        self.removePunctuations()  \n",
        "        self.removeQueryEXP()\n",
        "        self.spellCorrection()\n",
        "        # self.stemming()\n",
        "        # self.lemmatizer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Utils\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `Utils` class contains utility methods for set operations like intersection and union and proximity distance calculations that is a little custom function(maybe I should change it's name).\n",
        "\n",
        "#### Methods\n",
        "\n",
        "##### `interSect(ls: list)`\n",
        "\n",
        "- **Description**: Calculates the intersection of two lists.\n",
        "\n",
        "##### `union(ls: list)`\n",
        "\n",
        "- **Description**: Calculates the union of two lists.\n",
        "\n",
        "##### `complement(ls: list)`\n",
        "\n",
        "- **Description**: Calculates the complement of two sets.\n",
        "\n",
        "##### `proximateDistanceCal(ls: list, maxDistance: int)`\n",
        "\n",
        "- **Description**: Calculates proximate distances within the given maximum distance between indexes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Utils:\n",
        "    def __init__(self) -> None:\n",
        "        pass\n",
        "\n",
        "    def interSect(ls: list)->list:\n",
        "        return list(set([i for i in ls[0] if i in ls[1]]))\n",
        "    \n",
        "    def union(ls :list)->list:\n",
        "        return list(set(ls[0] + ls[1]))\n",
        "    \n",
        "    def complement(ls: list)->list:\n",
        "        return list(set([i for i in ls[1].values() if i not in ls[0]]))\n",
        "    \n",
        "    def proximateDistanceCal(ls: list, maxDistance: int)->list:\n",
        "        result = []\n",
        "        for docId, indexes in ls[0].items():\n",
        "            if(docId in ls[1]):\n",
        "                for i in range (len(indexes)-1):\n",
        "                    if(abs(indexes[i] - indexes[i+1]) <= maxDistance + 1):\n",
        "                        result.append(docId)\n",
        "                for i in range(len(ls[1][docId])-1):\n",
        "                    if(abs(ls[1][docId][i] - ls[1][docId][i+1]) <= maxDistance + 1):\n",
        "                        result.append(docId)     \n",
        "\n",
        "        return list(set(result))                        \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## IR System\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `IR` class is my information retrieval system that will generate InvertedInxe and will get your query and generate output, If you want a IR system that get many queries and be up for a while you can put start function in while.\n",
        "\n",
        "#### Methods\n",
        "\n",
        "##### `booleanSearch(query: QueryPreProcessing)`\n",
        "\n",
        "- **Description**: Performs boolean search operations (AND, OR, NOT) based on the provided query.\n",
        "\n",
        "##### `proximateSearch(query: QueryPreProcessing)`\n",
        "\n",
        "- **Description**: Performs proximate search operations based on the provided query and maximum distance.\n",
        "\n",
        "##### `wildcardSearch(self, queryTerm: str)`\n",
        "\n",
        "- **Description**: Performs serch on a term that is a wildcard query like $A^*B^*C$\n",
        "\n",
        "##### `search(query: QueryPreProcessing)`\n",
        "\n",
        "- **Description**: Determines the type of search operation (boolean or proximate) based on the query type and performs the search.\n",
        "\n",
        "##### `start()`\n",
        "\n",
        "- **Description**: Initiates the information retrieval process by taking user input for the query, processing it, and displaying the resulting document IDs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "XKg39uYw7ooB",
        "outputId": "e1c85b3e-b2a1-4945-fa37-694a12905a1a"
      },
      "outputs": [],
      "source": [
        "class IR:\n",
        "    def __init__(self) -> None:\n",
        "        self.indexes = Indexes()\n",
        "        self.indexes.build()\n",
        "        self.utils = Utils()\n",
        "\n",
        "    def booleanSearch(self, query: QueryPreProcessing)->list:\n",
        "        result_docs = []\n",
        "        for term in query.query:\n",
        "            if term.count(\"*\") == 0:\n",
        "                if term in self.indexes.invertedIndex:\n",
        "                    result_docs.append(list(self.indexes.invertedIndex[term].keys()))\n",
        "                else: result_docs.append([])\n",
        "            else:\n",
        "                result_docs.append(self.wildcardSearch(term))  \n",
        "         \n",
        "        if(query.queryType == \"AND\"): return Utils.interSect(result_docs)\n",
        "        elif (query.queryType == \"OR\") : return Utils.union(result_docs)\n",
        "        elif(query.queryType == \"NOT\") : \n",
        "            result_docs.append(InvertedIndex.DOC_ID_MAPPER)\n",
        "            return Utils.complement(result_docs)\n",
        "\n",
        "    def proximateSearch(self, query: QueryPreProcessing)->list:\n",
        "        result_docs = []\n",
        "        for term in query.query:\n",
        "            if term in self.indexes.invertedIndex:\n",
        "                result_docs.append(self.indexes.invertedIndex[term])\n",
        "\n",
        "        return Utils.proximateDistanceCal(result_docs, query.maxDistance)\n",
        "    \n",
        "    def wildcardSearch(self, queryTerm: str)->list:  \n",
        "        resultTerms = self.indexes.searchWildcard(queryTerm)\n",
        "        result_docs = []\n",
        "        for term in resultTerms:\n",
        "            if term in self.indexes.invertedIndex:\n",
        "               result_docs.extend(self.indexes.invertedIndex[term])\n",
        "        return list(set(result_docs))\n",
        "            \n",
        "    def search(self, query: QueryPreProcessing)-> list:\n",
        "        if query.queryType == \"spellCorrection\":\n",
        "            return query.query\n",
        "        elif query.queryType == \"wildCard\":\n",
        "            return self.wildcardSearch(query.query[0])\n",
        "        elif(query.queryType != \"NEAR\"):\n",
        "            return self.booleanSearch(query)\n",
        "        else:\n",
        "            return self.proximateSearch(query)   \n",
        "        \n",
        "    def start(self)->None:\n",
        "        while True:\n",
        "            query = input(\"Enter Your Query: '\\n'\") \n",
        "            if(query == \"end\"):\n",
        "                break  \n",
        "            print('\\n') \n",
        "            print(\"Input query : \" , query)\n",
        "            query = QueryPreProcessing(query, self.indexes.correctWords)\n",
        "            query.execute()\n",
        "            print(\"query Type: \", query.queryType)\n",
        "            print(\"query terms after pre process: \", query.query)\n",
        "            print(\"Result : \" , self.search(query))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "C29CtM9U7ooB"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0  ->  Jerry Decided To Buy a Gun.txt\n",
            "1  ->  Rentals at the Oceanside Community.txt\n",
            "2  ->  Gasoline Prices Hit Record High.txt\n",
            "3  ->  Cloning Pets.txt\n",
            "4  ->  Crazy Housing Prices.txt\n",
            "5  ->  Man Injured at Fast Food Place.txt\n",
            "6  ->  A Festival of Books.txt\n",
            "7  ->  Food Fight Erupted in Prison.txt\n",
            "8  ->  Better To Be Unlucky.txt\n",
            "9  ->  Sara Went Shopping.txt\n",
            "10  ->  Freeway Chase Ends at Newsstand.txt\n",
            "11  ->  Trees Are a Threat.txt\n",
            "12  ->  A Murder-Suicide.txt\n",
            "13  ->  Happy and Unhappy Renters.txt\n",
            "14  ->  Pulling Out Nine Tons of Trash.txt\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Input query :  jer*y\n",
            "query Type:  wildCard\n",
            "query terms after pre process:  ['jer*y']\n",
            "Result :  [0]\n",
            "\n",
            "\n",
            "Input query :  n*b*y\n",
            "query Type:  wildCard\n",
            "query terms after pre process:  ['n*b*y']\n",
            "Result :  [11, 6]\n",
            "\n",
            "\n",
            "Input query :  jer*y\n",
            "query Type:  wildCard\n",
            "query terms after pre process:  ['jer*y']\n",
            "Result :  [0]\n"
          ]
        }
      ],
      "source": [
        "IR = IR()\n",
        "IR.start()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Report:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Steps:\n",
        "\n",
        "##### 1. Initially, I retrieved my previous exercise codes, forming the foundation of the project.\n",
        "\n",
        "##### 2. I utilized the document preprocessor to generate a comprehensive list of correct words from the document corpus.\n",
        "\n",
        "##### 3. A robust spell correction mechanism was implemented using the Levenshtein distance algorithm. This ensured precise word correction based on the list of correct words.\n",
        "\n",
        "##### 4. Added new query types that we want.\n",
        "\n",
        "##### 5. A trie tree data structure was developed to facilitate the Permuterm Index, enabling seamless handling of wildcard queries.\n",
        "\n",
        "##### 6. The Permuterm Index was successfully implemented, addressing challenges posed by wildcard queries with multiple '\\*' symbols. Innovative solutions were applied in algorithm design.\n",
        "\n",
        "##### 7. All components were integrated, necessitating adjustments in the Information Retrieval (IR) system to handle wildcard and spell correction queries seamlessly.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## References:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. Our course slides\n",
        "2. https://www.nltk.org/\n",
        "3. https://chat.openai.com/\n",
        "4. https://nlp.stanford.edu/IR-book/html/htmledition/permuterm-indexes-1.html#:~:text=We%20refer%20to%20the%20set,query%20becomes%20n%24m*\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
